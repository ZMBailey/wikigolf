{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiGolf\n",
    "\n",
    "Goal: To reach a target page in as few links as possible\n",
    "\n",
    "Description:\n",
    "WikiGolf is a program intended to browse Wikipedia with some degree of intelligence. It will accept two parameters, a starting page and a target. It will begin at the starting page and check the links for anything related to the target, and if no relations are found it will choose one at random. It will then examine that page and check any of its links for pages related to the target, and so forth until it reaches a page that is within the threshold for relevence to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import wikipedia\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import wikisearch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.collocations\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "TITLE = 'Jurassic Park (novel)'\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': TITLE,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "#print(data['query']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ns': 0, 'title': '2000 AD (comics)'},\n",
       " {'ns': 0, 'title': 'A Case of Need'},\n",
       " {'ns': 0, 'title': 'Airframe (novel)'},\n",
       " {'ns': 0, 'title': 'Alfred A. Knopf'},\n",
       " {'ns': 0, 'title': 'Amazon (video game)'},\n",
       " {'ns': 0, 'title': 'Amber'},\n",
       " {'ns': 0, 'title': 'Amphibian'},\n",
       " {'ns': 0, 'title': 'Amusement park'},\n",
       " {'ns': 0, 'title': 'Ancient DNA'},\n",
       " {'ns': 0, 'title': 'Andrew Ferguson'},\n",
       " {'ns': 0, 'title': 'Auxotrophy'},\n",
       " {'ns': 0, 'title': 'BILBY Award'},\n",
       " {'ns': 0, 'title': 'Backdoor (computing)'},\n",
       " {'ns': 0, 'title': 'Barnes & Noble'},\n",
       " {'ns': 0, 'title': 'Battle at Big Rock'},\n",
       " {'ns': 0, 'title': 'Beyond Westworld'},\n",
       " {'ns': 0, 'title': 'Binary (novel)'},\n",
       " {'ns': 0, 'title': 'Biotechnology'},\n",
       " {'ns': 0, 'title': 'Bird'},\n",
       " {'ns': 0, 'title': 'Canopy Flyer'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['query']['pages'][list(data['query']['pages'])[0]]['links'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = data['query']['pages'][list(data['query']['pages'])[0]]['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ns': 0, 'title': 'DNA'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "links[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jurassic World: Original Motion Picture Soundtrack\n",
      "Jurassic World: Original Motion Picture Soundtrack\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "NEXT = links[n]['title']\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': NEXT,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "next_data = R.json()\n",
    "print(links[n]['title'])\n",
    "print(next_data['query']['pages'][list(next_data['query']['pages'])[0]]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(TITLE):\n",
    "    PARAMS = {\n",
    "        'action': \"query\",\n",
    "        'titles': TITLE,\n",
    "        'prop': \"links\",\n",
    "        'pllimit': \"max\",\n",
    "        'format': \"json\",\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    return R.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Jurassic Park (novel)\n",
      " 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Jurassic Park (novel)\n",
      "1 : Dragon curve\n",
      "2 : Blancmange curve\n",
      "3 : Archimedes\n",
      "4 : Menander\n",
      "5 : Akrai\n"
     ]
    }
   ],
   "source": [
    "start = input()\n",
    "hops = input()\n",
    "\n",
    "print(\"0 : \" + start)\n",
    "title = start\n",
    "for i in range(int(hops)):\n",
    "    response = get_links(title)\n",
    "    links = response['query']['pages'][list(response['query']['pages'])[0]]['links']\n",
    "    n = np.random.randint(0,len(links))\n",
    "    title = links[n]['title']\n",
    "    print(str(i+1) + \" : \" + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = wikipedia.page('Jurassic Park (novel)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine threshold for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gotham', 39),\n",
       " ('city', 31),\n",
       " ('batman', 27),\n",
       " ('land', 22),\n",
       " (\"man's\", 20),\n",
       " ('gordon', 17),\n",
       " ('story', 11),\n",
       " ('two', 11),\n",
       " ('comics', 10),\n",
       " ('joker', 9)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page(\"Batman: No Man's Land\"))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batman', 179),\n",
       " ('catwoman', 155),\n",
       " ('selina', 142),\n",
       " ('bruce', 50),\n",
       " ('two', 32),\n",
       " ('vol', 30),\n",
       " ('series', 29),\n",
       " ('story', 29),\n",
       " ('kyle', 28),\n",
       " ('one', 28)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page('Catwoman'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bread', 100),\n",
       " ('dough', 39),\n",
       " ('flour', 30),\n",
       " ('yeast', 25),\n",
       " ('leavening', 20),\n",
       " ('water', 18),\n",
       " ('breads', 18),\n",
       " ('used', 17),\n",
       " ('gluten', 14),\n",
       " ('wheat', 13),\n",
       " ('baking', 12),\n",
       " ('time', 12),\n",
       " ('made', 11),\n",
       " ('may', 11),\n",
       " ('protein', 11),\n",
       " ('process', 10),\n",
       " ('sourdough', 9),\n",
       " ('use', 9),\n",
       " ('also', 9),\n",
       " ('baked', 9)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page('Bread'))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cake', 75),\n",
       " ('cakes', 60),\n",
       " ('flour', 19),\n",
       " ('made', 14),\n",
       " ('sugar', 13),\n",
       " ('butter', 13),\n",
       " ('baking', 13),\n",
       " ('bread', 12),\n",
       " ('sponge', 12),\n",
       " ('baked', 11)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page('Cake'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spider', '0.06055'),\n",
       " ('parker', '0.01435'),\n",
       " ('comic', '0.01396'),\n",
       " ('peter', '0.01376'),\n",
       " ('charact', '0.01121'),\n",
       " ('amaz', '0.009632'),\n",
       " ('marvel', '0.008846'),\n",
       " ('issu', '0.008649'),\n",
       " ('superhero', '0.007273'),\n",
       " ('stori', '0.007273'),\n",
       " ('book', '0.007077'),\n",
       " ('seri', '0.006487'),\n",
       " ('origin', '0.005308'),\n",
       " ('time', '0.005308'),\n",
       " ('ditko', '0.005111'),\n",
       " ('also', '0.004914'),\n",
       " ('becom', '0.004718'),\n",
       " ('power', '0.004521'),\n",
       " ('best', '0.004325'),\n",
       " ('first', '0.004128')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisearch.normalized_top_50(wikipedia.page(\"Spider-man\").content)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = wikipedia.page('Batman').links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Holy...\"',\n",
       " '/Film',\n",
       " '100 (DC Comics)',\n",
       " '1939 in comics',\n",
       " '711 (Quality Comics)']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_links(current, target, visited):\n",
    "\n",
    "    #combine into list\n",
    "    page_text = current.content.split(\".\") + target.content.split(\".\")\n",
    "\n",
    "    text = []\n",
    "\n",
    "     #format for word2vec\n",
    "    for clue in page_text:\n",
    "        sentence = clue.translate(str.maketrans('','',string.punctuation)).split(' ')\n",
    "        new_sent = [word.lower() for word in sentence]   \n",
    "        text.append(new_sent)\n",
    "    \n",
    "    #create model\n",
    "    model = gensim.models.Word2Vec(text,sg=1)\n",
    "    model.train(text, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    #get links from current\n",
    "    #check links against model for relevence to target subject\n",
    "    links = current.links\n",
    "    success = []\n",
    "    errors = []\n",
    "    for l in links:\n",
    "        for word in l.split(' '):\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                dist = model.wv.distance(word,\"batman\")\n",
    "                if dist < 0.008 and l not in visited:\n",
    "                    success.append((l,dist))\n",
    "                    break\n",
    "            except KeyError:\n",
    "                errors.append(word)\n",
    "    \n",
    "    if len(success) > 0:\n",
    "        success.sort(key=lambda tup: tup[0])\n",
    "        return success[0][0]\n",
    "    else:\n",
    "        skiplist = ['Wikipedia', 'Category']\n",
    "        title = links[np.random.randint(0,len(links))]\n",
    "        while any(sub in title for sub in skiplist):\n",
    "            #print(title)\n",
    "            title = links[np.random.randint(0,len(links))]\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Jurassic Park (novel)\n",
      "..Not Found\n",
      "199 Hop(s), Finish at : Chambers of parliament\n"
     ]
    }
   ],
   "source": [
    "start = 'Jurassic Park (novel)'#input()\n",
    "target = 'Batman'#input()\n",
    "target_page = wikipedia.page(target)\n",
    "print(\"0 : \" + wikipedia.page(start).title)\n",
    "path = [start]\n",
    "visited = set(start)\n",
    "title = start\n",
    "exit = False\n",
    "for i in range(200):\n",
    "    try:\n",
    "        page = wikipedia.page(title)\n",
    "        top_20 = wikisearch.get_50_most_common(page.content)[:20]\n",
    "        for word in top_20:\n",
    "            if word[0] == target.lower():\n",
    "                exit = True\n",
    "        if exit:\n",
    "            break\n",
    "        search_success = True\n",
    "    except wikipedia.exceptions.DisambiguationError:\n",
    "        search_success = False\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        search_success = False\n",
    "        \n",
    "    title = check_links(page,target_page,visited)\n",
    "    visited.add(title)\n",
    "    path.append(title)\n",
    "    if i % 100 == 0:\n",
    "        print(\".\",end=\"\")\n",
    "\n",
    "print(\"Page Found!\") if exit else print(\"Not Found\")\n",
    "print(str(i) + \" Hop(s), Finish at : \" + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Case of Need',\n",
       " 'Abortion in the United States',\n",
       " 'Labor unions in the United States',\n",
       " 'Trade unions in Cuba',\n",
       " '1947–1948 Civil War in Mandatory Palestine',\n",
       " 'Operation Blacklist Forty',\n",
       " '1947–1949 Palestine war',\n",
       " 'Ndogboyosoi War',\n",
       " '1994 Bophuthatswana crisis',\n",
       " \"1946 African Mine Workers' Union strike\",\n",
       " '1948 South African general election',\n",
       " '1910 Cape of Good Hope Provincial Council election',\n",
       " '1910 South African general election',\n",
       " '1989 South African general election',\n",
       " '1915 South African general election',\n",
       " '1917 Cape of Good Hope Provincial Council election',\n",
       " '1920 South African general election',\n",
       " '1921 South African general election',\n",
       " '1921 in South Africa',\n",
       " '1901 in South Africa',\n",
       " '1898 in South Africa',\n",
       " '1895 in South Africa',\n",
       " '1892 in South Africa',\n",
       " '1889 in South Africa',\n",
       " '1886 in South Africa',\n",
       " '1883 in South Africa',\n",
       " '1880 in South Africa',\n",
       " '1877 in South Africa',\n",
       " '1874 in South Africa',\n",
       " '1871 in South Africa',\n",
       " '1868 in South Africa',\n",
       " '1865 in South Africa',\n",
       " '1862 in South Africa',\n",
       " '1859 in South Africa',\n",
       " '1856 in South Africa',\n",
       " '1853 in South Africa',\n",
       " '1850 in South Africa',\n",
       " '1847 in South Africa',\n",
       " '1844 in South Africa',\n",
       " '1841 in South Africa',\n",
       " '1838 in South Africa',\n",
       " '1835 in South Africa',\n",
       " '1832 in South Africa',\n",
       " '1829 in South Africa',\n",
       " '1826 in South Africa',\n",
       " '1823 in South Africa',\n",
       " '1820 in South Africa',\n",
       " '1817 in South Africa',\n",
       " '1814 in South Africa',\n",
       " '1811 in South Africa',\n",
       " '1808 in South Africa',\n",
       " '1805 in South Africa',\n",
       " '1802 in South Africa',\n",
       " '1799 in South Africa',\n",
       " 'Music of South Africa',\n",
       " 'Music',\n",
       " 'Joseph Haydn',\n",
       " 'Christian Cannabich',\n",
       " 'Age of Enlightenment',\n",
       " 'Jewish philosophy',\n",
       " 'Jewish prayer',\n",
       " 'Yisrael Meir Kagan',\n",
       " 'American Jew',\n",
       " 'San Mateo County, California',\n",
       " 'San Leandro Bay',\n",
       " 'China Camp State Park',\n",
       " 'Barbecue in the United States',\n",
       " 'Alta California',\n",
       " 'Battle of Dominguez Rancho',\n",
       " 'Action of Atlixco',\n",
       " 'Action of Sequalteplan',\n",
       " 'Affair at Galaxara Pass',\n",
       " 'Battle for Mexico City',\n",
       " 'Battle of Buena Vista',\n",
       " 'Antonio López de Santa Anna',\n",
       " 'Juan Seguín',\n",
       " 'A Martinez',\n",
       " 'ALMA Awards',\n",
       " '11th ALMA Awards',\n",
       " 'Adam Rodriguez',\n",
       " 'About Last Night (2014 film)',\n",
       " 'Academic grading in the United States',\n",
       " 'Academic grading in North America',\n",
       " 'University of Calgary',\n",
       " 'University of Strathclyde',\n",
       " 'Academic Ranking of World Universities',\n",
       " 'Academic rankings of universities in Mexico',\n",
       " 'Anáhuac University',\n",
       " 'Anahuac Universities Network',\n",
       " 'Anahuac Mayab University',\n",
       " 'Center for Higher Studies',\n",
       " 'Lima',\n",
       " '131st IOC Session',\n",
       " '2024 Summer Olympics',\n",
       " '2024 Summer Olympic and Paralympic mascots',\n",
       " '2016 Summer Paralympics',\n",
       " 'Deodoro Stadium',\n",
       " '2016 Summer Olympics',\n",
       " 'List of athletes at the 2016 Summer Olympics with a prior doping offence',\n",
       " '2016 Summer Olympics Parade of Nations']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page1 = wikipedia.page('Bread')\n",
    "page2 = wikipedia.page('Batman')\n",
    "page_text = page1.content.split(\".\") + page2.content.split(\".\")\n",
    "\n",
    "text = []\n",
    "\n",
    "for clue in page_text:\n",
    "    sentence = clue.translate(str.maketrans('','',string.punctuation)).split(' ')\n",
    "    new_sent = [word.lower() for word in sentence]   \n",
    "    text.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bread',\n",
       "  'is',\n",
       "  'a',\n",
       "  'staple',\n",
       "  'food',\n",
       "  'prepared',\n",
       "  'from',\n",
       "  'a',\n",
       "  'dough',\n",
       "  'of',\n",
       "  'flour',\n",
       "  'and',\n",
       "  'water',\n",
       "  'usually',\n",
       "  'by',\n",
       "  'baking'],\n",
       " ['',\n",
       "  'throughout',\n",
       "  'recorded',\n",
       "  'history',\n",
       "  'it',\n",
       "  'has',\n",
       "  'been',\n",
       "  'a',\n",
       "  'prominent',\n",
       "  'food',\n",
       "  'in',\n",
       "  'large',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  'and',\n",
       "  'is',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'oldest',\n",
       "  'manmade',\n",
       "  'foods',\n",
       "  'having',\n",
       "  'been',\n",
       "  'of',\n",
       "  'significant',\n",
       "  'importance',\n",
       "  'since',\n",
       "  'the',\n",
       "  'dawn',\n",
       "  'of',\n",
       "  'agriculture']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(text,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36979, 88430)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13668"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0086287260055542"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance(\"baked\",\"batman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWB Limited', 'Alfred A. Knopf', 'Bosnia and Herzegovina cuisine', 'Bread and circuses', 'Bread and salt', 'Bread in Europe', 'Bread in culture', 'Cuisine of São Tomé and Príncipe', 'Cuisine of the Americas', 'Cuisine of the Central African Republic', 'Cuisine of the Indian subcontinent', 'Cuisine of the Mizrahi Jews', 'Cuisine of the Pennsylvania Dutch', 'Cuisine of the Sephardic Jews', 'Cuisine of the Thirteen Colonies', 'Culture of Himachal Pradesh', 'Democratic Republic of the Congo cuisine', 'Diet in Hinduism', 'Diet in Sikhism', 'Early modern European cuisine', 'English cuisine', 'English language', 'Eurasian cuisine of Singapore and Malaysia', 'Food and Agriculture Organization', 'Gliadin', 'History of Chinese cuisine', 'History of agriculture', 'History of bread', 'History of cuisine from the Indian subcontinent', 'History of seafood', 'History of vegetarianism', 'Indigenous cuisine of the Americas', 'International Standard Book Number', 'List of brand name breads', 'List of countries by wheat exports', 'List of cuisines of the Americas', 'List of dishes from the Caucasus', 'Lists of prepared foods', 'Middle English', 'Modern English', 'National Archives and Records Administration', 'Natufian culture', 'New American cuisine', 'New Zealand cuisine', 'Note by Note cuisine', 'Old English language', 'Old High German', \"Old wives' tale\", 'Oxford English Dictionary', 'Peasant foods', 'Pliny the Elder', 'Pure culture', 'Recorded history', 'Red Fife wheat', 'Sponge and dough', 'The Independent', 'Trinidad and Tobago cuisine', 'West African cuisine', 'West Frisian language', 'Wheat pools in Canada']\n"
     ]
    }
   ],
   "source": [
    "links1 = page1.links\n",
    "success = []\n",
    "errors = []\n",
    "for l in links1:\n",
    "    for word in l.split(' '):\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            if model.wv.distance(word,\"batman\") < 0.008:\n",
    "                success.append(l)\n",
    "                break\n",
    "        except KeyError:\n",
    "            errors.append(word)\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "710\n"
     ]
    }
   ],
   "source": [
    "print(len(success))\n",
    "print(len(links1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\ntwoface', 0.9981786012649536),\n",
       " ('video', 0.997878909111023),\n",
       " ('animated', 0.9978392720222473),\n",
       " ('universe', 0.9977647066116333),\n",
       " ('television', 0.9977205991744995),\n",
       " ('movie', 0.9977039098739624),\n",
       " ('film', 0.9976729154586792),\n",
       " ('liveaction', 0.9976227879524231),\n",
       " ('series', 0.997494637966156),\n",
       " ('first', 0.9974403977394104)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"batman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Holy...\"',\n",
       " '711 (Quality Comics)',\n",
       " 'A Death Worse Than Fate',\n",
       " 'Abin Sur',\n",
       " 'Ace Magazines (comics)',\n",
       " 'Ace the Bat-Hound',\n",
       " 'Action Comics',\n",
       " 'African-American',\n",
       " 'Air Wave',\n",
       " 'Airboy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_page = wikipedia.page('Catwoman')\n",
    "cw_links = cw_page.links\n",
    "cw_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holi']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_stopped = [wikisearch.preprocess(cw_page.content)]\n",
    "words_stopped[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(words_stopped)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in words_stopped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda.print_topics(-1):\n",
    "    print(type(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page = wikipedia.page('Baking')\n",
    "test_words = wikisearch.preprocess(test_page)\n",
    "tdictionary = gensim.corpora.Dictionary([test_words])\n",
    "bow_test = [dictionary.doc2bow(test_words)]\n",
    "test = lda[bow_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.interfaces.TransformedCorpus"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for topic in test:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
