{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import wikipedia\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "TITLE = 'Jurassic Park (novel)'\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': TITLE,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "#print(data['query']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ns': 0, 'title': '2000 AD (comics)'},\n",
       " {'ns': 0, 'title': 'A Case of Need'},\n",
       " {'ns': 0, 'title': 'Airframe (novel)'},\n",
       " {'ns': 0, 'title': 'Alfred A. Knopf'},\n",
       " {'ns': 0, 'title': 'Amazon (video game)'},\n",
       " {'ns': 0, 'title': 'Amber'},\n",
       " {'ns': 0, 'title': 'Amphibian'},\n",
       " {'ns': 0, 'title': 'Amusement park'},\n",
       " {'ns': 0, 'title': 'Ancient DNA'},\n",
       " {'ns': 0, 'title': 'Andrew Ferguson'},\n",
       " {'ns': 0, 'title': 'Auxotrophy'},\n",
       " {'ns': 0, 'title': 'BILBY Award'},\n",
       " {'ns': 0, 'title': 'Backdoor (computing)'},\n",
       " {'ns': 0, 'title': 'Barnes & Noble'},\n",
       " {'ns': 0, 'title': 'Battle at Big Rock'},\n",
       " {'ns': 0, 'title': 'Beyond Westworld'},\n",
       " {'ns': 0, 'title': 'Binary (novel)'},\n",
       " {'ns': 0, 'title': 'Biotechnology'},\n",
       " {'ns': 0, 'title': 'Bird'},\n",
       " {'ns': 0, 'title': 'Canopy Flyer'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['query']['pages'][list(data['query']['pages'])[0]]['links'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = data['query']['pages'][list(data['query']['pages'])[0]]['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ns': 0, 'title': 'DNA'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "links[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jurassic World: Original Motion Picture Soundtrack\n",
      "Jurassic World: Original Motion Picture Soundtrack\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "NEXT = links[n]['title']\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': NEXT,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "next_data = R.json()\n",
    "print(links[n]['title'])\n",
    "print(next_data['query']['pages'][list(next_data['query']['pages'])[0]]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(TITLE):\n",
    "    PARAMS = {\n",
    "        'action': \"query\",\n",
    "        'titles': TITLE,\n",
    "        'prop': \"links\",\n",
    "        'pllimit': \"max\",\n",
    "        'format': \"json\",\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    return R.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Jurassic Park (novel)\n",
      " 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Jurassic Park (novel)\n",
      "1 : Dragon curve\n",
      "2 : Blancmange curve\n",
      "3 : Archimedes\n",
      "4 : Menander\n",
      "5 : Akrai\n"
     ]
    }
   ],
   "source": [
    "start = input()\n",
    "hops = input()\n",
    "\n",
    "print(\"0 : \" + start)\n",
    "title = start\n",
    "for i in range(int(hops)):\n",
    "    response = get_links(title)\n",
    "    links = response['query']['pages'][list(response['query']['pages'])[0]]['links']\n",
    "    n = np.random.randint(0,len(links))\n",
    "    title = links[n]['title']\n",
    "    print(str(i+1) + \" : \" + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = wikipedia.page('Jurassic Park (novel)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.collocations\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_50_most_common(page):\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokens_raw = nltk.regexp_tokenize(page.content, pattern)\n",
    "    tokens = [word.lower() for word in tokens_raw]\n",
    "\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += list(string.punctuation)\n",
    "    stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "    words_stopped = [word for word in tokens if word not in stopwords_list]\n",
    "\n",
    "    freqdist = FreqDist(words_stopped)\n",
    "    return freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine threshold for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gotham', 39),\n",
       " ('city', 31),\n",
       " ('batman', 27),\n",
       " ('land', 22),\n",
       " (\"man's\", 20),\n",
       " ('gordon', 17),\n",
       " ('story', 11),\n",
       " ('two', 11),\n",
       " ('comics', 10),\n",
       " ('joker', 9)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page(\"Batman: No Man's Land\"))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batman', 179),\n",
       " ('catwoman', 155),\n",
       " ('selina', 142),\n",
       " ('bruce', 50),\n",
       " ('two', 32),\n",
       " ('vol', 30),\n",
       " ('series', 29),\n",
       " ('story', 29),\n",
       " ('kyle', 28),\n",
       " ('one', 28)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page('Catwoman'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bread', 100),\n",
       " ('dough', 39),\n",
       " ('flour', 30),\n",
       " ('yeast', 25),\n",
       " ('leavening', 20),\n",
       " ('water', 18),\n",
       " ('breads', 18),\n",
       " ('used', 17),\n",
       " ('gluten', 14),\n",
       " ('wheat', 13),\n",
       " ('baking', 12),\n",
       " ('time', 12),\n",
       " ('made', 11),\n",
       " ('may', 11),\n",
       " ('protein', 11),\n",
       " ('process', 10),\n",
       " ('sourdough', 9),\n",
       " ('use', 9),\n",
       " ('also', 9),\n",
       " ('baked', 9)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page('Bread'))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cake', 75),\n",
       " ('cakes', 60),\n",
       " ('flour', 19),\n",
       " ('made', 14),\n",
       " ('sugar', 13),\n",
       " ('butter', 13),\n",
       " ('baking', 13),\n",
       " ('bread', 12),\n",
       " ('sponge', 12),\n",
       " ('baked', 11)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_50_most_common(wikipedia.page('Cake'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_top_50(page):\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokens_raw = nltk.regexp_tokenize(page.content, pattern)\n",
    "    tokens = [word.lower() for word in tokens_raw]\n",
    "\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += list(string.punctuation)\n",
    "    stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "    words_stopped = [word for word in tokens if word not in stopwords_list]\n",
    "\n",
    "    freqdist = FreqDist(words_stopped)\n",
    "    top_50 = freqdist.most_common(50)\n",
    "    \n",
    "    total = sum(freqdist.values())\n",
    "    \n",
    "    normalized = []\n",
    "    for word in top_50:\n",
    "        normalized_frequency = word[1] / total\n",
    "        normalized.append((word[0],\"{:.4}\".format(normalized_frequency)))\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gotham', '0.028'),\n",
       " ('city', '0.02225'),\n",
       " ('batman', '0.01938'),\n",
       " ('land', '0.01579'),\n",
       " (\"man's\", '0.01436'),\n",
       " ('gordon', '0.0122'),\n",
       " ('story', '0.007897'),\n",
       " ('two', '0.007897'),\n",
       " ('comics', '0.007179'),\n",
       " ('joker', '0.006461'),\n",
       " ('storyline', '0.005743'),\n",
       " ('gang', '0.005743'),\n",
       " ('police', '0.005743'),\n",
       " ('also', '0.005025'),\n",
       " ('territory', '0.005025'),\n",
       " ('dc', '0.004307'),\n",
       " ('issues', '0.004307'),\n",
       " ('new', '0.004307'),\n",
       " ('face', '0.004307'),\n",
       " ('luthor', '0.004307')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_top_50(wikipedia.page(\"Batman: No Man's Land\"))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Superman\n",
      " Batman\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Superman\n",
      "."
     ]
    },
    {
     "ename": "DisambiguationError",
     "evalue": "\"Scratch\" may refer to: \nScratch (musician)\nLee \"Scratch\" Perry\nScratch (robot)\nGobots\nScratch (2001 film)\nScratch (2008 film)\nScratch (2010 film)\nScratch (2015 film)\nScratch band\nScratch Orchestra\nScratch Track\nScratch (Kenny Barron album)\nScratch (Kaela Kimura album)\nScratch (soundtrack)\nThe Crusaders\nPeter Gabriel (1978 album)\nYes\nThe Surfaris\ndampened\nScratching\nScratchboard\ngraffiti\nScratch (magazine)\nScratch Radio\nScratches (video game)\nScratch (programming language)\nScratch space\nmud fever\nrecrystallization\nScratch (horse)\nScratch\nScratch\nhandicap\nScratch race\nScratch track (disambiguation)\nmoney\nOld Scratch\nScratch building\nScratchcard\nScratchings\nScratchpad (disambiguation)\nFrom scratch (disambiguation)\nSgraffito",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDisambiguationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-cd9dc8aab9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mexit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtop_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_50_most_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[0;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self, redirect, preload)\u001b[0m\n\u001b[1;32m    391\u001b[0m       \u001b[0mmay_refer_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_lis\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisambiguationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmay_refer_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDisambiguationError\u001b[0m: \"Scratch\" may refer to: \nScratch (musician)\nLee \"Scratch\" Perry\nScratch (robot)\nGobots\nScratch (2001 film)\nScratch (2008 film)\nScratch (2010 film)\nScratch (2015 film)\nScratch band\nScratch Orchestra\nScratch Track\nScratch (Kenny Barron album)\nScratch (Kaela Kimura album)\nScratch (soundtrack)\nThe Crusaders\nPeter Gabriel (1978 album)\nYes\nThe Surfaris\ndampened\nScratching\nScratchboard\ngraffiti\nScratch (magazine)\nScratch Radio\nScratches (video game)\nScratch (programming language)\nScratch space\nmud fever\nrecrystallization\nScratch (horse)\nScratch\nScratch\nhandicap\nScratch race\nScratch track (disambiguation)\nmoney\nOld Scratch\nScratch building\nScratchcard\nScratchings\nScratchpad (disambiguation)\nFrom scratch (disambiguation)\nSgraffito"
     ]
    }
   ],
   "source": [
    "start = input()\n",
    "target = input()\n",
    "\n",
    "skiplist = ['Wikipedia', 'Category']\n",
    "\n",
    "print(\"0 : \" + start)\n",
    "title = start\n",
    "exit = False\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        page = wikipedia.page(title)\n",
    "        top_20 = get_50_most_common(page)[:20]\n",
    "        for word in top_20:\n",
    "            if word[0] == target:\n",
    "                exit = True\n",
    "        if exit:\n",
    "            break\n",
    "        response = get_links(title)\n",
    "        links = response['query']['pages'][list(response['query']['pages'])[0]]['links']\n",
    "        search_success = True\n",
    "    except wikipedia.exceptions.DisambiguationError:\n",
    "        self.ambiguation.append(page)\n",
    "        search_success = False\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        self.pageerrors.append(page)\n",
    "        search_success = False\n",
    "    if search_success:\n",
    "\n",
    "    n = np.random.randint(0,len(links))\n",
    "    title = links[n]['title']\n",
    "    while any(sub in title for sub in skiplist):\n",
    "        n = np.random.randint(0,len(links))\n",
    "        title = links[n]['title']\n",
    "    if i % 100 == 0:\n",
    "        print(\".\",end=\"\")\n",
    "print(str(i+1) + \" Hops, Finish at : \" + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wikipedia.page('Two-Face')\n",
    "page_text = page.content.split(\".\")\n",
    "\n",
    "text = []\n",
    "\n",
    "for clue in page_text:\n",
    "    sentence = clue.translate(str.maketrans('','',string.punctuation)).split(' ')\n",
    "    new_sent = []\n",
    "    for word in sentence:\n",
    "        new_sent.append(word.lower())\n",
    "        \n",
    "    text.append(new_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['twoface',\n",
       "  'harvey',\n",
       "  'dent',\n",
       "  'is',\n",
       "  'a',\n",
       "  'fictional',\n",
       "  'character',\n",
       "  'appearing',\n",
       "  'in',\n",
       "  'comic',\n",
       "  'books',\n",
       "  'published',\n",
       "  'by',\n",
       "  'dc',\n",
       "  'comics',\n",
       "  'commonly',\n",
       "  'as',\n",
       "  'an',\n",
       "  'adversary',\n",
       "  'of',\n",
       "  'the',\n",
       "  'superhero',\n",
       "  'batman'],\n",
       " ['',\n",
       "  'the',\n",
       "  'character',\n",
       "  'was',\n",
       "  'created',\n",
       "  'by',\n",
       "  'bob',\n",
       "  'kane',\n",
       "  '',\n",
       "  'and',\n",
       "  'bill',\n",
       "  'finger',\n",
       "  'and',\n",
       "  'first',\n",
       "  'appeared',\n",
       "  'in',\n",
       "  'detective',\n",
       "  'comics',\n",
       "  '66',\n",
       "  'august',\n",
       "  '1942']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(text,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26991, 68340)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13668"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\ntwoface', 0.9981786012649536),\n",
       " ('video', 0.997878909111023),\n",
       " ('animated', 0.9978392720222473),\n",
       " ('universe', 0.9977647066116333),\n",
       " ('television', 0.9977205991744995),\n",
       " ('movie', 0.9977039098739624),\n",
       " ('film', 0.9976729154586792),\n",
       " ('liveaction', 0.9976227879524231),\n",
       " ('series', 0.997494637966156),\n",
       " ('first', 0.9974403977394104)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"batman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
