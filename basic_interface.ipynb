{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiGolf\n",
    "\n",
    "Goal: To reach a target page in as few links as possible\n",
    "\n",
    "Description:\n",
    "WikiGolf is a program intended to browse Wikipedia with some degree of intelligence. It will accept two parameters, a starting page and a target. It will begin at the starting page and check the links for anything related to the target, and if no relations are found it will choose one at random. It will then examine that page and check any of its links for pages related to the target, and so forth until it reaches a page that is within the threshold for relevence to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import wikipedia\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import wikisearch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.collocations\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual interaction with the Wikipedia API\n",
    "\n",
    "Experimenting with obtaining all the links in a page through direct API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "TITLE = 'Jurassic Park (novel)'\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': TITLE,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "#print(data['query']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ns': 0, 'title': '2000 AD (comics)'},\n",
       " {'ns': 0, 'title': 'A Case of Need'},\n",
       " {'ns': 0, 'title': 'Airframe (novel)'},\n",
       " {'ns': 0, 'title': 'Alfred A. Knopf'},\n",
       " {'ns': 0, 'title': 'Amazon (video game)'},\n",
       " {'ns': 0, 'title': 'Amber'},\n",
       " {'ns': 0, 'title': 'Amphibian'},\n",
       " {'ns': 0, 'title': 'Amusement park'},\n",
       " {'ns': 0, 'title': 'Ancient DNA'},\n",
       " {'ns': 0, 'title': 'Andrew Ferguson'},\n",
       " {'ns': 0, 'title': 'Auxotrophy'},\n",
       " {'ns': 0, 'title': 'BILBY Award'},\n",
       " {'ns': 0, 'title': 'Backdoor (computing)'},\n",
       " {'ns': 0, 'title': 'Barnes & Noble'},\n",
       " {'ns': 0, 'title': 'Battle at Big Rock'},\n",
       " {'ns': 0, 'title': 'Beyond Westworld'},\n",
       " {'ns': 0, 'title': 'Binary (novel)'},\n",
       " {'ns': 0, 'title': 'Biotechnology'},\n",
       " {'ns': 0, 'title': 'Bird'},\n",
       " {'ns': 0, 'title': 'Canopy Flyer'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['query']['pages'][list(data['query']['pages'])[0]]['links'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = data['query']['pages'][list(data['query']['pages'])[0]]['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ns': 0, 'title': 'DNA'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "links[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jurassic World: Original Motion Picture Soundtrack\n",
      "Jurassic World: Original Motion Picture Soundtrack\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "NEXT = links[n]['title']\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': NEXT,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "next_data = R.json()\n",
    "print(links[n]['title'])\n",
    "print(next_data['query']['pages'][list(next_data['query']['pages'])[0]]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(TITLE):\n",
    "    PARAMS = {\n",
    "        'action': \"query\",\n",
    "        'titles': TITLE,\n",
    "        'prop': \"links\",\n",
    "        'pllimit': \"max\",\n",
    "        'format': \"json\",\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    return R.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Jurassic Park (novel)\n",
      " 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Jurassic Park (novel)\n",
      "1 : Dragon curve\n",
      "2 : Blancmange curve\n",
      "3 : Archimedes\n",
      "4 : Menander\n",
      "5 : Akrai\n"
     ]
    }
   ],
   "source": [
    "start = input()\n",
    "hops = input()\n",
    "\n",
    "print(\"0 : \" + start)\n",
    "title = start\n",
    "for i in range(int(hops)):\n",
    "    response = get_links(title)\n",
    "    links = response['query']['pages'][list(response['query']['pages'])[0]]['links']\n",
    "    n = np.random.randint(0,len(links))\n",
    "    title = links[n]['title']\n",
    "    print(str(i+1) + \" : \" + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = wikipedia.page('Jurassic Park (novel)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with custom built functinos using FreqDist calls from the NLTK package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gotham', 41),\n",
       " ('citi', 38),\n",
       " ('batman', 33),\n",
       " ('land', 24),\n",
       " ('man', 21),\n",
       " ('gordon', 20),\n",
       " ('comic', 13),\n",
       " ('storylin', 11),\n",
       " ('stori', 11),\n",
       " ('gang', 11)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page(\"Batman: No Man's Land\").content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batman', 179),\n",
       " ('catwoman', 155),\n",
       " ('selina', 142),\n",
       " ('bruce', 50),\n",
       " ('two', 32),\n",
       " ('vol', 30),\n",
       " ('series', 29),\n",
       " ('story', 29),\n",
       " ('kyle', 28),\n",
       " ('one', 28)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Catwoman').content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bread', 100),\n",
       " ('dough', 39),\n",
       " ('flour', 30),\n",
       " ('yeast', 25),\n",
       " ('leavening', 20),\n",
       " ('water', 18),\n",
       " ('breads', 18),\n",
       " ('used', 17),\n",
       " ('gluten', 14),\n",
       " ('wheat', 13),\n",
       " ('baking', 12),\n",
       " ('time', 12),\n",
       " ('made', 11),\n",
       " ('may', 11),\n",
       " ('protein', 11),\n",
       " ('process', 10),\n",
       " ('sourdough', 9),\n",
       " ('use', 9),\n",
       " ('also', 9),\n",
       " ('baked', 9)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Bread').content)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cake', 75),\n",
       " ('cakes', 60),\n",
       " ('flour', 19),\n",
       " ('made', 14),\n",
       " ('sugar', 13),\n",
       " ('butter', 13),\n",
       " ('baking', 13),\n",
       " ('bread', 12),\n",
       " ('sponge', 12),\n",
       " ('baked', 11)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Cake').content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spider', '0.06055'),\n",
       " ('parker', '0.01435'),\n",
       " ('comic', '0.01396'),\n",
       " ('peter', '0.01376'),\n",
       " ('charact', '0.01121'),\n",
       " ('amaz', '0.009632'),\n",
       " ('marvel', '0.008846'),\n",
       " ('issu', '0.008649'),\n",
       " ('superhero', '0.007273'),\n",
       " ('stori', '0.007273'),\n",
       " ('book', '0.007077'),\n",
       " ('seri', '0.006487'),\n",
       " ('origin', '0.005308'),\n",
       " ('time', '0.005308'),\n",
       " ('ditko', '0.005111'),\n",
       " ('also', '0.004914'),\n",
       " ('becom', '0.004718'),\n",
       " ('power', '0.004521'),\n",
       " ('best', '0.004325'),\n",
       " ('first', '0.004128')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisearch.normalized_top_50(wikipedia.page(\"Spider-man\").content)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = wikipedia.page('Trickster (comics)')\n",
    "t = wikipedia.page('Catwoman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 0.998737633228302),\n",
       " ('1', 0.9985352158546448),\n",
       " ('becomes', 0.998490035533905),\n",
       " ('annual', 0.998481273651123),\n",
       " ('elseworlds', 0.9984744787216187),\n",
       " ('batman', 0.9984369874000549),\n",
       " ('vol', 0.9984265565872192),\n",
       " ('2011', 0.9983789920806885),\n",
       " ('age', 0.9983401894569397),\n",
       " ('continuity', 0.9982714653015137)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_model(c,t)\n",
    "\n",
    "dist = model.wv.distance('also'.lower(),'Catwoman'.lower())\n",
    "# for word in c.title.split(' '):\n",
    "#             word = word.lower()\n",
    "#             try:\n",
    "#                 for targetword in t.title.split(' '):\n",
    "#                     dist = model.wv.distance(word,targetword.lower())\n",
    "#                     if dist < 0.05:\n",
    "#                         print(\"True\")\n",
    "#                         break\n",
    "#             except KeyError:\n",
    "#                 print(\"KeyError: \" + c.title)\n",
    "# if dist < 0.009:\n",
    "#         print(\"True\")\n",
    "\n",
    "model.wv.most_similar(['catwoman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015254616737365723"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('catwoman', 'elseworlds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(current,target):\n",
    "    #combine into list\n",
    "    page_text = current.content.split(\".\") + target.content.split(\".\")\n",
    "\n",
    "    text = []\n",
    "\n",
    "     #format for word2vec\n",
    "    for clue in page_text:\n",
    "        sentence = clue.translate(str.maketrans('','',string.punctuation)).split(' ')\n",
    "        new_sent = [word.lower() for word in sentence]   \n",
    "        text.append(new_sent)\n",
    "    \n",
    "    #create model\n",
    "    model = gensim.models.Word2Vec(text,sg=1)\n",
    "    model.train(text, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "def check_links(model,current,target,visited):\n",
    "\n",
    "    #model = make_model(current,target)\n",
    "    \n",
    "    #get links from current\n",
    "    #check links against model for relevence to target subject\n",
    "    links = current.links\n",
    "    success = []\n",
    "    errors = []\n",
    "    for l in links:\n",
    "        for word in l.split(' '):\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                for targetword in target.split(' '):\n",
    "                    dist = model.wv.distance(word,targetword.lower())\n",
    "                    if dist < 0.02 and l not in visited:\n",
    "                        success.append((l,dist))\n",
    "                        break\n",
    "            except KeyError:\n",
    "                errors.append(word)\n",
    "    \n",
    "    if len(success) > 0:\n",
    "        success.sort(key=lambda tup: tup[1])\n",
    "        return success[0][0]\n",
    "    else:\n",
    "        skiplist = ['Wikipedia', 'Category']\n",
    "        title = links[np.random.randint(0,len(links))]\n",
    "        while any(sub in title for sub in skiplist):\n",
    "            #print(title)\n",
    "            title = links[np.random.randint(0,len(links))]\n",
    "        print(title)\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Jurassic Park\n",
      ".Page Found!\n",
      "8 Hop(s), Finish at : Batman\n",
      "Word: also with dist: 0.0019004344940185547\n"
     ]
    }
   ],
   "source": [
    "start = input()\n",
    "target = input()\n",
    "target_page = wikipedia.page(target)\n",
    "closest = (\"none\",sys.maxsize)\n",
    "print(\"0 : \" + wikipedia.page(start).title)\n",
    "path = [start]\n",
    "visited = set(start)\n",
    "title = start\n",
    "exit = False\n",
    "for i in range(100):\n",
    "    try:\n",
    "        if title.lower() == target.lower():\n",
    "            exit = True\n",
    "            break\n",
    "        current = wikipedia.page(title)\n",
    "        model = make_model(current,target_page)\n",
    "        top_20 = wikisearch.get_50_most_common(page.content)[:20]\n",
    "#         for word in top_20:\n",
    "#             if word == target.lower():\n",
    "# #             for t in target.split(' '):\n",
    "# #                 if model.wv.distance(t,word):\n",
    "        for word,freq in top_20:\n",
    "            try:\n",
    "                for target_word in wikisearch.preprocess(target_page.title):\n",
    "                    dist = model.wv.distance(word,target_word)\n",
    "                    if dist < 0.0016 and closest[1] < dist:\n",
    "                            closest = (title, dist)\n",
    "#                         exit = True\n",
    "#                         break\n",
    "            except KeyError:\n",
    "                pass\n",
    "#             if exit:\n",
    "#                 break\n",
    "#         if exit:\n",
    "#             break\n",
    "        search_success = True\n",
    "    except wikipedia.exceptions.DisambiguationError:\n",
    "        search_success = False\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        search_success = False\n",
    "        \n",
    "    title = check_links(model,current,target,visited)\n",
    "    visited.add(title)\n",
    "    path.append(title)\n",
    "    if i % 100 == 0:\n",
    "        print(\".\",end=\"\")\n",
    "\n",
    "print(\"Page Found!\") if exit else print(\"Not Found, closest page was: \" + closest[0])\n",
    "print(str(i) + \" Hop(s), Finish at : \" + title)\n",
    "print(\"Word: \" + word + \" with dist: \" + str(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = make_model(wikipedia.page('Batman'),wikipedia.page('Superman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02975165843963623"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.wv.distance('luthor','superman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page1 = wikipedia.page('Bread')\n",
    "page2 = wikipedia.page('Batman')\n",
    "page_text = page1.content.split(\".\") + page2.content.split(\".\")\n",
    "\n",
    "text = []\n",
    "\n",
    "for clue in page_text:\n",
    "    sentence = clue.translate(str.maketrans('','',string.punctuation)).split(' ')\n",
    "    new_sent = [word.lower() for word in sentence]   \n",
    "    text.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-50f3c185cf30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0020514726638793945 < 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('also','catwoman') < 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jurassic park'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Jurassic Park\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
