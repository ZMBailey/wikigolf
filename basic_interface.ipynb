{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiGolf\n",
    "\n",
    "Goal: To reach a target page in as few links as possible\n",
    "\n",
    "Description:\n",
    "WikiGolf is a program intended to browse Wikipedia with some degree of intelligence. It will accept two parameters, a starting page and a target. It will begin at the starting page and check the links for anything related to the target, and if no relations are found it will choose one at random. It will then examine that page and check any of its links for pages related to the target, and so forth until it reaches a page that is within the threshold for relevence to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import wikipedia\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import wikisearch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.collocations\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual interaction with the Wikipedia API\n",
    "\n",
    "It is possible to find all links on a page using direct interaction with the Wikipedia API, but accessing the relevent information from the query result is somewhat complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "TITLE = 'Jurassic Park (novel)'\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': TITLE,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "#print(data['query']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['query']['pages'][list(data['query']['pages'])[0]]['links'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = data['query']['pages'][list(data['query']['pages'])[0]]['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "links[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "NEXT = links[n]['title']\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': NEXT,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "next_data = R.json()\n",
    "print(links[n]['title'])\n",
    "print(next_data['query']['pages'][list(next_data['query']['pages'])[0]]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(TITLE):\n",
    "    PARAMS = {\n",
    "        'action': \"query\",\n",
    "        'titles': TITLE,\n",
    "        'prop': \"links\",\n",
    "        'pllimit': \"max\",\n",
    "        'format': \"json\",\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    return R.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = input()\n",
    "hops = input()\n",
    "\n",
    "print(\"0 : \" + start)\n",
    "title = start\n",
    "for i in range(int(hops)):\n",
    "    response = get_links(title)\n",
    "    links = response['query']['pages'][list(response['query']['pages'])[0]]['links']\n",
    "    n = np.random.randint(0,len(links))\n",
    "    title = links[n]['title']\n",
    "    print(str(i+1) + \" : \" + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `Wikipedia` wrapper for the API we can retrieve the same information with a much more simple call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = wikipedia.page('Jurassic Park (novel)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have created a function in `wikisearch` which uses a FreqDist to find the 50 most common words in a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page(\"Batman: No Man's Land\").content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Catwoman').content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Bread').content)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Cake').content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.normalized_top_50(wikipedia.page(\"Spider-man\").content)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = wikipedia.page('Trickster (comics)')\n",
    "t = wikipedia.page('Catwoman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(c,t)\n",
    "\n",
    "dist = model.wv.distance('also'.lower(),'Catwoman'.lower())\n",
    "# for word in c.title.split(' '):\n",
    "#             word = word.lower()\n",
    "#             try:\n",
    "#                 for targetword in t.title.split(' '):\n",
    "#                     dist = model.wv.distance(word,targetword.lower())\n",
    "#                     if dist < 0.05:\n",
    "#                         print(\"True\")\n",
    "#                         break\n",
    "#             except KeyError:\n",
    "#                 print(\"KeyError: \" + c.title)\n",
    "# if dist < 0.009:\n",
    "#         print(\"True\")\n",
    "\n",
    "model.wv.most_similar(['catwoman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('catwoman', 'elseworlds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program\n",
    "\n",
    "Test functions and main program, for ease of testing and making adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(current,target):\n",
    "    #combine into list\n",
    "    page_text = current.content.split(\".\") + target.content.split(\".\")\n",
    "\n",
    "    text = []\n",
    "\n",
    "     #format for word2vec\n",
    "    for clue in page_text:\n",
    "        sentence = clue.translate(str.maketrans('','',string.punctuation)).split(' ')\n",
    "        new_sent = [word.lower() for word in sentence]   \n",
    "        text.append(new_sent)\n",
    "    \n",
    "    #create model\n",
    "    model = gensim.models.Word2Vec(text,sg=1)\n",
    "    model.train(text, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "def check_links(model,current,target,visited):\n",
    "    \n",
    "    #get links from current\n",
    "    links = current.links\n",
    "    success = []\n",
    "    errors = []\n",
    "    \n",
    "    #check links against model for relevence to target subject\n",
    "    for l in links:\n",
    "        for word in l.split(' '):\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                for targetword in target.split(' '):\n",
    "                    dist = model.wv.distance(word,targetword.lower())\n",
    "                    if dist < 0.02 and l not in visited:\n",
    "                        success.append((l,dist))\n",
    "                        break\n",
    "            except KeyError:\n",
    "                errors.append(word)\n",
    "    #if related links found, use most related link, otherwise random\n",
    "    if len(success) > 0:\n",
    "        success.sort(key=lambda tup: tup[1])\n",
    "        return success[0][0]\n",
    "    else:\n",
    "        skiplist = ['Wikipedia', 'Category']\n",
    "        title = links[np.random.randint(0,len(links))]\n",
    "        while any(sub in title for sub in skiplist):\n",
    "            title = links[np.random.randint(0,len(links))]\n",
    "        print(title)\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get input\n",
    "start = input()\n",
    "target = input()\n",
    "#set up initial variables\n",
    "target_page = wikipedia.page(target)\n",
    "closest = (\"none\",sys.maxsize)\n",
    "print(\"0 : \" + wikipedia.page(start).title)\n",
    "path = [start]\n",
    "visited = set(start)\n",
    "title = start\n",
    "exit = False\n",
    "i = 0\n",
    "\n",
    "#for i in range(100):\n",
    "while not exit:\n",
    "    try:\n",
    "        #test for target page\n",
    "        if title.lower() == target.lower():\n",
    "            exit = True\n",
    "            break\n",
    "        #get next page\n",
    "        current = wikipedia.page(title)\n",
    "        #create model\n",
    "        model = make_model(current,target_page)\n",
    "        #get common words in current page\n",
    "        top_20 = wikisearch.get_50_most_common(page.content)[:20]\n",
    "#         for word in top_20:\n",
    "#             if word == target.lower():\n",
    "# #             for t in target.split(' '):\n",
    "# #                 if model.wv.distance(t,word):\n",
    "        #test for similarity to target page\n",
    "        for word,freq in top_20:\n",
    "            try:\n",
    "                for target_word in wikisearch.preprocess(target_page.title):\n",
    "                    dist = model.wv.distance(word,target_word)\n",
    "                    if dist < 0.0016 and closest[1] < dist:\n",
    "                            closest = (title, dist)\n",
    "#                         exit = True\n",
    "#                         break\n",
    "            except KeyError:\n",
    "                pass\n",
    "#             if exit:\n",
    "#                 break\n",
    "#         if exit:\n",
    "#             break\n",
    "        search_success = True\n",
    "    except wikipedia.exceptions.DisambiguationError:\n",
    "        search_success = False\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        search_success = False\n",
    "        \n",
    "    #get next link\n",
    "    title = check_links(model,current,target,visited)\n",
    "    visited.add(title)\n",
    "    path.append(title)\n",
    "    if i % 100 == 0:\n",
    "        print(\".\",end=\"\")\n",
    "    i += 1\n",
    "\n",
    "print(\"Page Found!\") if exit else print(\"Not Found, closest page was: \" + closest[0])\n",
    "print(str(i) + \" Hop(s), Finish at : \" + title)\n",
    "#print(\"Word: \" + word + \" with dist: \" + str(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = make_model(wikipedia.page('Batman'),wikipedia.page('Superman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.wv.distance('luthor','superman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page1 = wikipedia.page('Bread')\n",
    "page2 = wikipedia.page('Batman')\n",
    "page_text = page1.content.split(\".\") + page2.content.split(\".\")\n",
    "\n",
    "text = []\n",
    "\n",
    "for clue in page_text:\n",
    "    sentence = clue.translate(str.maketrans('','',string.punctuation)).split(' ')\n",
    "    new_sent = [word.lower() for word in sentence]   \n",
    "    text.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0020514726638793945 < 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('also','catwoman') < 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Jurassic Park\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
