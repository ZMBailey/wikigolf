{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiGolf\n",
    "\n",
    "Goal: To reach a target page in as few links as possible\n",
    "\n",
    "Description:\n",
    "WikiGolf is a program intended to browse Wikipedia with some degree of intelligence. It will accept two parameters, a starting page and a target. It will begin at the starting page and check the links for anything related to the target, and if no relations are found it will choose one at random. It will then examine that page and check any of its links for pages related to the target, and so forth until it reaches a page that is within the threshold for relevence to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import wikipedia\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import wikisearch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.collocations\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual interaction with the Wikipedia API\n",
    "\n",
    "It is possible to find all links on a page using direct interaction with the Wikipedia API, but accessing the relevent information from the query result is somewhat complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "TITLE = 'Jurassic Park (novel)'\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': TITLE,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "#print(data['query']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['query']['pages'][list(data['query']['pages'])[0]]['links'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = data['query']['pages'][list(data['query']['pages'])[0]]['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "links[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(0,len(links))\n",
    "NEXT = links[n]['title']\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'titles': NEXT,\n",
    "    'prop': \"links\",\n",
    "    'pllimit': \"max\",\n",
    "    'format': \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "next_data = R.json()\n",
    "print(links[n]['title'])\n",
    "print(next_data['query']['pages'][list(next_data['query']['pages'])[0]]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(TITLE):\n",
    "    PARAMS = {\n",
    "        'action': \"query\",\n",
    "        'titles': TITLE,\n",
    "        'prop': \"links\",\n",
    "        'pllimit': \"max\",\n",
    "        'format': \"json\",\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    return R.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = input()\n",
    "hops = input()\n",
    "\n",
    "print(\"0 : \" + start)\n",
    "title = start\n",
    "for i in range(int(hops)):\n",
    "    response = get_links(title)\n",
    "    links = response['query']['pages'][list(response['query']['pages'])[0]]['links']\n",
    "    n = np.random.randint(0,len(links))\n",
    "    title = links[n]['title']\n",
    "    print(str(i+1) + \" : \" + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `Wikipedia` wrapper for the API we can retrieve the same information with a much more simple call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = wikipedia.page('Jurassic Park (novel)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have created a function in `wikisearch` which uses a FreqDist to find the 50 most common words in a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page(\"Batman: No Man's Land\").content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Catwoman').content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Bread').content)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.get_50_most_common(wikipedia.page('Cake').content)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisearch.normalized_top_50(wikipedia.page(\"Spider-man\").content)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = wikipedia.page('Trickster (comics)')\n",
    "t = wikipedia.page('Catwoman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `wikisearch` I have created a function to make a Word2Vec model based on 2 documents, which will be used to create models based on the target page and the current page, in order to compare links to the target subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(c,t)\n",
    "\n",
    "dist = model.wv.distance('also'.lower(),'Catwoman'.lower())\n",
    "\n",
    "model.wv.most_similar(['catwoman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('catwoman', 'elseworlds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program\n",
    "\n",
    "The main program for running WikiGolf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Batman\n",
      " Jurassic Park\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Batman\n",
      "..........Not Found, closest page was: none\n",
      "99 Hop(s), Finish at : Lincoln Park Zoo\n"
     ]
    }
   ],
   "source": [
    "#get input\n",
    "start = input()\n",
    "target = input()\n",
    "#set up initial variables\n",
    "target_page = wikipedia.page(target)\n",
    "closest = (\"none\",sys.maxsize)\n",
    "print(\"0 : \" + wikipedia.page(start).title)\n",
    "# if 'path' in locals():\n",
    "#     if target_page not in path:\n",
    "path = [target_page]\n",
    "visited = set()\n",
    "visited.add(start)\n",
    "title = start\n",
    "exit = False\n",
    "#i = 0\n",
    "\n",
    "for i in range(100):\n",
    "#while not exit:\n",
    "    try:\n",
    "        #test for target page\n",
    "        if title.lower() == target.lower():\n",
    "            exit = True\n",
    "            break\n",
    "        #get next page\n",
    "        current = wikipedia.page(title)\n",
    "        #if current not in path:\n",
    "        path.append(current)\n",
    "        #create model\n",
    "        model = wikisearch.make_model_new(path)\n",
    "        #get common words in current page\n",
    "        top_20 = wikisearch.get_50_most_common(current.content)[:20]\n",
    "\n",
    "        #test for similarity to target page\n",
    "        for word,freq in top_20:\n",
    "            try:\n",
    "                for target_word in wikisearch.preprocess(target_page.title):\n",
    "                    dist = model.wv.distance(word,target_word)\n",
    "                    if closest[1] < dist:\n",
    "                            closest = (title, dist)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        search_success = True\n",
    "    except wikipedia.exceptions.DisambiguationError:\n",
    "        search_success = False\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        search_success = False\n",
    "        \n",
    "    #get next link\n",
    "    title = wikisearch.check_links(model,current,target,visited)\n",
    "    visited.add(title)\n",
    "    if i % 10 == 0:\n",
    "        print(\".\",end=\"\")\n",
    "    #i += 1\n",
    "\n",
    "print(\"Page Found!\") if exit else print(\"Not Found, closest page was: \" + closest[0])\n",
    "print(str(i) + \" Hop(s), Finish at : \" + title)\n",
    "#print(\"Word: \" + word + \" with dist: \" + str(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = make_model(wikipedia.page('Batman'),wikipedia.page('Superman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.wv.distance('luthor','superman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Jurassic Park\n"
     ]
    }
   ],
   "source": [
    "title, found = wikisearch.run_golf('Batman','Jurassic Park')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<WikipediaPage 'Saint Seiya'>,\n",
       " <WikipediaPage 'Dragon Ball'>,\n",
       " <WikipediaPage 'Saint (manhua)'>,\n",
       " <WikipediaPage 'Journey to the West – Legends of the Monkey King'>,\n",
       " <WikipediaPage 'Gao Village Arc'>,\n",
       " <WikipediaPage 'Xin (comics)'>,\n",
       " <WikipediaPage 'Harris Publications'>,\n",
       " <WikipediaPage 'Guitar World'>,\n",
       " <WikipediaPage 'Play (UK magazine)'>,\n",
       " <WikipediaPage 'PlayStation'>,\n",
       " <WikipediaPage 'Scuzz'>,\n",
       " <WikipediaPage 'Starz'>,\n",
       " <WikipediaPage 'Matt's Monsters'>,\n",
       " <WikipediaPage 'Cartoon Network'>,\n",
       " <WikipediaPage 'Jim Samples'>,\n",
       " <WikipediaPage 'Disney Channel'>,\n",
       " <WikipediaPage 'Middle East'>,\n",
       " <WikipediaPage 'Sanaa'>,\n",
       " <WikipediaPage 'South Arabia'>,\n",
       " <WikipediaPage 'Upper Yafa'>,\n",
       " <WikipediaPage 'Al-Busi'>,\n",
       " <WikipediaPage 'Federation of Arab Emirates of the South'>,\n",
       " <WikipediaPage 'Kathiri'>,\n",
       " <WikipediaPage 'Wahidi Haban'>,\n",
       " <WikipediaPage 'Upper Yafa'>,\n",
       " <WikipediaPage 'Wahidi Bir Ali'>,\n",
       " <WikipediaPage 'Bi'r `Ali'>,\n",
       " <WikipediaPage 'Time zone'>,\n",
       " <WikipediaPage 'Collectivity of Saint Martin'>,\n",
       " <WikipediaPage '2007 Saint Martin Territorial Council election'>,\n",
       " <WikipediaPage '2012 Saint Martin Territorial Council election'>,\n",
       " <WikipediaPage 'Politics of the Collectivity of Saint Martin'>,\n",
       " <WikipediaPage 'List of political parties in the Collectivity of Saint Martin'>,\n",
       " <WikipediaPage 'Democratic Alliance for Saint Martin'>,\n",
       " <WikipediaPage 'Collectivity of Saint Martin'>,\n",
       " <WikipediaPage 'Attack on Saint Martin'>,\n",
       " <WikipediaPage 'Capture of Saint Martin (1633)'>,\n",
       " <WikipediaPage 'Saint Martin'>,\n",
       " <WikipediaPage 'Culture of Saint Martin'>,\n",
       " <WikipediaPage 'Saint Barthélemy'>,\n",
       " <WikipediaPage '1877 Saint Barthélemy status referendum'>,\n",
       " <WikipediaPage '2007 Saint Barthélemy Territorial Council election'>,\n",
       " <WikipediaPage '2012 Saint Barthélemy Territorial Council election'>,\n",
       " <WikipediaPage 'Territorial Council of Saint Barthélemy'>,\n",
       " <WikipediaPage 'Gustavia, Saint Barthélemy'>,\n",
       " <WikipediaPage 'List of lighthouses in Saint Barthélemy'>,\n",
       " <WikipediaPage 'List of lighthouses in Saint Kitts and Nevis'>,\n",
       " <WikipediaPage 'List of lighthouses in Saint Lucia'>,\n",
       " <WikipediaPage 'List of lighthouses in Saint Pierre and Miquelon'>,\n",
       " <WikipediaPage 'List of lighthouses in Saint Vincent and the Grenadines'>,\n",
       " <WikipediaPage 'Charlotte Parish, Saint Vincent and the Grenadines'>,\n",
       " <WikipediaPage 'List of cities, towns and villages in Saint Vincent and the Grenadines'>,\n",
       " <WikipediaPage 'Arnos Vale, Saint Vincent and the Grenadines'>,\n",
       " <WikipediaPage 'Kingstown'>,\n",
       " <WikipediaPage 'Fort Charlotte, Saint Vincent'>,\n",
       " <WikipediaPage 'Capture of Saint Vincent'>,\n",
       " <WikipediaPage 'Battle of Saint Kitts'>,\n",
       " <WikipediaPage 'Saint Kitts'>,\n",
       " <WikipediaPage 'History of Saint Lucia'>,\n",
       " <WikipediaPage 'Saint Barthélemy'>,\n",
       " <WikipediaPage 'Church of Saint Barbara, Valletta'>,\n",
       " <WikipediaPage 'Church of Saint Catherine of Italy, Valletta'>,\n",
       " <WikipediaPage 'Church of Saint John the Baptist, Jerusalem'>,\n",
       " <WikipediaPage 'Cathedral of Saint James, Jerusalem'>,\n",
       " <WikipediaPage 'Saint George's Church, Tbilisi'>,\n",
       " <WikipediaPage 'Chapel of Saint Helena, Jerusalem'>,\n",
       " <WikipediaPage 'Church of Saint Anne, Jerusalem'>,\n",
       " <WikipediaPage 'James Intercisus'>,\n",
       " <WikipediaPage 'Monastery of Saint James the Mutilated (Qara)'>,\n",
       " <WikipediaPage 'Monastery of Saint Moses the Abyssinian'>,\n",
       " <WikipediaPage 'Monastery of Saint John of Dailam'>,\n",
       " <WikipediaPage 'Ain Sifni'>,\n",
       " <WikipediaPage 'Iran'>,\n",
       " <WikipediaPage 'Saint Stepanos Monastery'>,\n",
       " <WikipediaPage 'Church of Saint George (Kldisubani)'>,\n",
       " <WikipediaPage 'Church of Saint John, Mastara'>,\n",
       " <WikipediaPage 'Church of Saint Toros'>,\n",
       " <WikipediaPage 'Church of Saint Mary of the Germans'>,\n",
       " <WikipediaPage 'Church of Saint Mary of the Latins'>,\n",
       " <WikipediaPage 'Monastery of Saint Mark, Jerusalem'>,\n",
       " <WikipediaPage 'Monastery of Saint Saviour'>,\n",
       " <WikipediaPage 'Oriental Orthodox Churches'>,\n",
       " <WikipediaPage 'Anaphora of Saint Gregory'>,\n",
       " <WikipediaPage 'Liturgy of Saint Basil'>,\n",
       " <WikipediaPage 'Liturgy of Saint Cyril'>,\n",
       " <WikipediaPage 'Liturgy of Saint John Chrysostom'>,\n",
       " <WikipediaPage 'List of Eastern Orthodox saint titles'>,\n",
       " <WikipediaPage 'Apostolic succession'>,\n",
       " <WikipediaPage 'Latter Day Saint movement'>,\n",
       " <WikipediaPage 'Bibliography of the Latter Day Saint movement'>,\n",
       " <WikipediaPage 'History of the Latter Day Saint movement'>,\n",
       " <WikipediaPage 'Mormonism and Freemasonry'>,\n",
       " <WikipediaPage 'Latter Day Saint movement'>,\n",
       " <WikipediaPage 'List of denominations in the Latter Day Saint movement'>,\n",
       " <WikipediaPage 'Billy Johnson (Mormon)'>,\n",
       " <WikipediaPage 'Provisional National Defence Council'>,\n",
       " <WikipediaPage '2019 Gabonese coup d'état attempt'>,\n",
       " <WikipediaPage '1974 Nigerien coup d'état'>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
